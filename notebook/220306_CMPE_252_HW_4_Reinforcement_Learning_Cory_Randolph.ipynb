{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "220306_CMPE_252_HW_4_Reinforcement_Learning_Cory_Randolph",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coryroyce/reinforcement_learning_open_ai_gym/blob/main/notebook/220306_CMPE_252_HW_4_Reinforcement_Learning_Cory_Randolph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8LmF1GL2kx8"
      },
      "source": [
        "# Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Cory Randolph\n",
        "\n",
        "3/7/2022"
      ],
      "metadata": {
        "id": "o69yQnyo7Na7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAYjnlUQ2xwg"
      },
      "source": [
        "# Prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing Q- Learning in OpenAI gym (40p)\n",
        "* A car is on a one-dimensional track, positioned between two “mountains”. The goal is to drive up the mountain on the right; however, the car’s engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum.\n"
      ],
      "metadata": {
        "id": "SS8NoWkLzuhS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning Overview"
      ],
      "metadata": {
        "id": "3Nb82p0x0bs-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg_yo2cZ0btG"
      },
      "source": [
        "The below image is a diagram showing the main components of Reinforcement Learning from [sadiakhaf](https://github.com/sadiakhaf/IEEE-Hands-On-RL-using-Python)\n",
        "\n",
        "*   What is an **environment** in RL? \n",
        "*   And What is an **agent**?\n",
        "*   What is *state*?\n",
        "*   What is an *action*?\n",
        "*   What is *reward*?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adUcOc9u0btG"
      },
      "source": [
        "![picture](https://drive.google.com/uc?export=view&id=13oYKs5qWbpPekxMQN5ExG2kLo4ih4pKS)\n",
        "\n",
        "\n",
        "https://drive.google.com/file/d/13oYKs5qWbpPekxMQN5ExG2kLo4ih4pKS/view?usp=sharing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4nTrjhQ0btG"
      },
      "source": [
        "Bais process overview of RL\n",
        "\n",
        "1. REST() env and get state\n",
        "2.   Give this state to *agent*, wait for him to *act* --> ACT()\n",
        "3.   Give his *action* to env and get *reward* --> STEP()\n",
        "4.   Pass this reward to *agent* for his behavior, make him learn --> UPDATE(). Plot something if you need to.\n",
        "5.   Go to step 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npDyNpPkdIil"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the needed packages"
      ],
      "metadata": {
        "id": "gg5uny6JuxII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required system dependencies\n",
        "!apt-get install -y xvfb x11-utils\n",
        "\n",
        "# Install required python dependencies (might need to install additional gym extras depending)\n",
        "!pip install pyvirtualdisplay PyOpenGL PyOpenGL-accelerate\n",
        "\n",
        "# Install ML libraries \n",
        "!pip install tensorflow #==2.3.0\n",
        "!pip install gym\n",
        "!pip install keras\n",
        "!pip install keras-rl2\n",
        "\n",
        "\n",
        "# Clear output for this cell\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "PkYnqP8NyXM3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up the correct virtual display settings to run fully in Colab"
      ],
      "metadata": {
        "id": "ykl_gwLjystd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyvirtualdisplay\n",
        "\n",
        "_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
        "                                    size=(1400, 900))\n",
        "_ = _display.start()"
      ],
      "metadata": {
        "id": "xqa5m0wRymqw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twf9FpWZ0btH"
      },
      "source": [
        "# Gym Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Gym"
      ],
      "metadata": {
        "id": "tkJ154VBjyhv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Gym"
      ],
      "metadata": {
        "id": "wm8fOfwm0btH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym \n",
        "import random"
      ],
      "metadata": {
        "id": "RqawIuT9dD1A"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the environment and need components"
      ],
      "metadata": {
        "id": "JxSdEpQIeo3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load gym environment \n",
        "env_name = 'MountainCar-v0'\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# Update the display to render the video as a file (then download from Colab)\n",
        "video_every_n_episodes = 5\n",
        "env = gym.wrappers.Monitor(env, \"./video\", video_callable=lambda episode_id: (episode_id%video_every_n_episodes)==0, force=True)\n",
        "\n",
        "# Extract the states and actions\n",
        "states = env.observation_space.shape[0]\n",
        "actions = env.action_space.n"
      ],
      "metadata": {
        "id": "f0q5OVvzdG7o"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate some random actions just to verify the environment is setup correctly"
      ],
      "metadata": {
        "id": "XApWNqZCexhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = 10\n",
        "for episode in range(1, episodes+1):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    score = 0 \n",
        "    \n",
        "    while not done:\n",
        "        env.render()\n",
        "        action = random.choice([0,1])\n",
        "        n_state, reward, done, info = env.step(action)\n",
        "        score+=reward\n",
        "    print('Episode:{} Score:{}'.format(episode, score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2k4STAtc1zI",
        "outputId": "e6173bde-a97b-480a-f19c-b321973a01b4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode:1 Score:-200.0\n",
            "Episode:2 Score:-200.0\n",
            "Episode:3 Score:-200.0\n",
            "Episode:4 Score:-200.0\n",
            "Episode:5 Score:-200.0\n",
            "Episode:6 Score:-200.0\n",
            "Episode:7 Score:-200.0\n",
            "Episode:8 Score:-200.0\n",
            "Episode:9 Score:-200.0\n",
            "Episode:10 Score:-200.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Reinforcement Learning Model"
      ],
      "metadata": {
        "id": "dGXKRWBEf6PR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build RL Model"
      ],
      "metadata": {
        "id": "YFixC5NuCzH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "4ovt4Hqogmdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Activation\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "NZ9KAcGDgmdv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define how the model should be built"
      ],
      "metadata": {
        "id": "3sbwvakPgmdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Random state\n",
        "np.random.seed(3)\n",
        "env.seed(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEQ67d54gxPm",
        "outputId": "5f912ba3-1733-462b-b00b-997cc13fd295"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(states, actions):\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "    model.add(Dense(100))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(100))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(actions, activation='linear'))\n",
        "    return model"
      ],
      "metadata": {
        "id": "N4cdqfGKgmdv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(states, actions)"
      ],
      "metadata": {
        "id": "4qsI5RILgmdw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display model architecture"
      ],
      "metadata": {
        "id": "uZ_ywox4gmdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7f446ac-42a7-4d86-bb32-c0a0e4a801ce",
        "id": "FuHQ99zYgmdw"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 2)                 0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 100)               300       \n",
            "                                                                 \n",
            " activation (Activation)     (None, 100)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 100)               10100     \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 100)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 3)                 303       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10,703\n",
            "Trainable params: 10,703\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build RL Agent"
      ],
      "metadata": {
        "id": "xU6B1xRDiOl1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Keras Reinforcement learning library to create and train a RL model"
      ],
      "metadata": {
        "id": "qdXN45uuiOl9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Keras RL libraries"
      ],
      "metadata": {
        "id": "6GPUfqgQiOl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory"
      ],
      "metadata": {
        "id": "eDS0sMVmiOl9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the Agent"
      ],
      "metadata": {
        "id": "kVIDIZBUiOl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_agent(model, actions):\n",
        "    policy = EpsGreedyQPolicy(eps=.1)\n",
        "    memory = SequentialMemory(limit=50_000, window_length=1)\n",
        "    dqn = DQNAgent(model=model, \n",
        "                   memory=memory, \n",
        "                   policy=policy, \n",
        "                   nb_actions=actions, \n",
        "                   nb_steps_warmup=10,\n",
        "                   enable_dueling_network=True, # Dueling network increase performance by splitting the value and advantage into 2 outputs\n",
        "                   dueling_type='avg',\n",
        "                   target_model_update=1e-2,)\n",
        "    return dqn"
      ],
      "metadata": {
        "id": "t6K5U_e3iOl-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove any old model before compiling"
      ],
      "metadata": {
        "id": "0iJHic1fkTVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del model"
      ],
      "metadata": {
        "id": "GiwjXZx4kSb7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(states, actions)"
      ],
      "metadata": {
        "id": "fVVrLdjlkScD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dqn = build_agent(model, actions)\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b511ebc2-db45-4d74-a10a-3d4826242641",
        "id": "NST4xv-8kScE"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fit Model"
      ],
      "metadata": {
        "id": "ZsJDgiHMtV7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit the model by training for a number of steps.\n",
        "\n",
        "Since fitting the model takes a while to run through the 200,000 steps, you can skip the training steps by loading the saved model weights in the code below."
      ],
      "metadata": {
        "id": "YHBqkTfakScE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_steps=200_000\n",
        "dqn.fit(env, nb_steps=nb_steps, visualize=False, verbose=1) #nb_steps=500_000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9de9262-6ce1-43f5-a303-e0a915023479",
        "id": "Apcurb4wkScE"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 200000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "    5/10000 [..............................] - ETA: 2:08 - reward: -1.0000 "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r    9/10000 [..............................] - ETA: 2:08 - reward: -1.0000"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000/10000 [==============================] - 115s 12ms/step - reward: -1.0000\n",
            "50 episodes - episode_reward: -198.000 [-200.000, -144.000] - loss: 2.387 - mae: 20.056 - mean_q: -29.677\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 116s 12ms/step - reward: -1.0000\n",
            "55 episodes - episode_reward: -180.636 [-200.000, -105.000] - loss: 5.305 - mae: 32.976 - mean_q: -48.674\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: -1.0000\n",
            "64 episodes - episode_reward: -158.234 [-200.000, -88.000] - loss: 3.417 - mae: 30.763 - mean_q: -45.245\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 117s 12ms/step - reward: -1.0000\n",
            "68 episodes - episode_reward: -146.088 [-200.000, -95.000] - loss: 2.402 - mae: 29.736 - mean_q: -43.736\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 118s 12ms/step - reward: -1.0000\n",
            "66 episodes - episode_reward: -151.667 [-200.000, -112.000] - loss: 2.175 - mae: 30.200 - mean_q: -44.387\n",
            "\n",
            "Interval 6 (50000 steps performed)\n",
            "10000/10000 [==============================] - 121s 12ms/step - reward: -1.0000\n",
            "68 episodes - episode_reward: -146.882 [-186.000, -91.000] - loss: 1.297 - mae: 30.263 - mean_q: -44.460\n",
            "\n",
            "Interval 7 (60000 steps performed)\n",
            "10000/10000 [==============================] - 120s 12ms/step - reward: -1.0000\n",
            "70 episodes - episode_reward: -143.229 [-200.000, -88.000] - loss: 0.598 - mae: 29.899 - mean_q: -43.867\n",
            "\n",
            "Interval 8 (70000 steps performed)\n",
            "10000/10000 [==============================] - 120s 12ms/step - reward: -1.0000\n",
            "79 episodes - episode_reward: -126.924 [-176.000, -92.000] - loss: 0.233 - mae: 29.439 - mean_q: -43.153\n",
            "\n",
            "Interval 9 (80000 steps performed)\n",
            "10000/10000 [==============================] - 121s 12ms/step - reward: -1.0000\n",
            "79 episodes - episode_reward: -126.190 [-197.000, -88.000] - loss: 0.167 - mae: 28.672 - mean_q: -41.893\n",
            "\n",
            "Interval 10 (90000 steps performed)\n",
            "10000/10000 [==============================] - 124s 12ms/step - reward: -1.0000\n",
            "77 episodes - episode_reward: -130.883 [-200.000, -96.000] - loss: 0.186 - mae: 27.835 - mean_q: -40.507\n",
            "\n",
            "Interval 11 (100000 steps performed)\n",
            "10000/10000 [==============================] - 126s 13ms/step - reward: -1.0000\n",
            "78 episodes - episode_reward: -126.962 [-189.000, -88.000] - loss: 0.144 - mae: 27.270 - mean_q: -39.647\n",
            "\n",
            "Interval 12 (110000 steps performed)\n",
            "10000/10000 [==============================] - 127s 13ms/step - reward: -1.0000\n",
            "84 episodes - episode_reward: -119.333 [-200.000, -91.000] - loss: 0.162 - mae: 26.689 - mean_q: -38.696\n",
            "\n",
            "Interval 13 (120000 steps performed)\n",
            "10000/10000 [==============================] - 126s 13ms/step - reward: -1.0000\n",
            "74 episodes - episode_reward: -134.419 [-200.000, -92.000] - loss: 0.142 - mae: 26.199 - mean_q: -37.973\n",
            "\n",
            "Interval 14 (130000 steps performed)\n",
            "10000/10000 [==============================] - 122s 12ms/step - reward: -1.0000\n",
            "75 episodes - episode_reward: -133.893 [-200.000, -85.000] - loss: 0.146 - mae: 26.382 - mean_q: -38.252\n",
            "\n",
            "Interval 15 (140000 steps performed)\n",
            "10000/10000 [==============================] - 122s 12ms/step - reward: -1.0000\n",
            "75 episodes - episode_reward: -133.667 [-187.000, -87.000] - loss: 0.171 - mae: 26.548 - mean_q: -38.386\n",
            "\n",
            "Interval 16 (150000 steps performed)\n",
            "10000/10000 [==============================] - 122s 12ms/step - reward: -1.0000\n",
            "74 episodes - episode_reward: -135.757 [-200.000, -85.000] - loss: 0.180 - mae: 26.917 - mean_q: -38.872\n",
            "\n",
            "Interval 17 (160000 steps performed)\n",
            "10000/10000 [==============================] - 120s 12ms/step - reward: -1.0000\n",
            "79 episodes - episode_reward: -126.177 [-199.000, -91.000] - loss: 0.189 - mae: 27.004 - mean_q: -38.951\n",
            "\n",
            "Interval 18 (170000 steps performed)\n",
            "10000/10000 [==============================] - 123s 12ms/step - reward: -1.0000\n",
            "81 episodes - episode_reward: -122.975 [-200.000, -84.000] - loss: 0.191 - mae: 26.764 - mean_q: -38.506\n",
            "\n",
            "Interval 19 (180000 steps performed)\n",
            "10000/10000 [==============================] - 125s 13ms/step - reward: -1.0000\n",
            "80 episodes - episode_reward: -125.138 [-200.000, -85.000] - loss: 0.187 - mae: 26.587 - mean_q: -38.246\n",
            "\n",
            "Interval 20 (190000 steps performed)\n",
            "10000/10000 [==============================] - 123s 12ms/step - reward: -1.0000\n",
            "done, took 2420.746 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f85653d2cd0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save and Load Weights"
      ],
      "metadata": {
        "id": "PnaHx4sxta7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the weights of the trained model"
      ],
      "metadata": {
        "id": "XQnwUusjl8Bs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# After training is done, we save the final weights.\n",
        "dqn.save_weights(f'dqn_{nb_steps}_steps_{env_name}_weights.h5f', overwrite=True)"
      ],
      "metadata": {
        "id": "0vCQfHq8lbNF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Delete the current model to verify that we are fully loading from the trained weights"
      ],
      "metadata": {
        "id": "jxwocK-1zXza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "del dqn\n",
        "del env"
      ],
      "metadata": {
        "id": "Rowmo4pHzXzb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup the model Architecture the same as before so that the weights can be loaded back in."
      ],
      "metadata": {
        "id": "FR7bQhM4zXzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(env_name)\n",
        "env = gym.wrappers.Monitor(env, \"./video\", video_callable=lambda episode_id: episode_id, force=True)\n",
        "actions = env.action_space.n\n",
        "states = env.observation_space.shape[0]\n",
        "model = build_model(states, actions)\n",
        "dqn = build_agent(model, actions)\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a38742f1-7101-417b-bc58-5119ee32d52c",
        "id": "zfH9aEtIzXzb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load previously saved weights just to test\n",
        "dqn.load_weights(f'dqn_{nb_steps}_steps_{env_name}_weights.h5f')"
      ],
      "metadata": {
        "id": "LG1-RkERlbNN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Model"
      ],
      "metadata": {
        "id": "_G8j9xV8te4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the display to render the video as a file (then download from Colab)\n",
        "del env\n",
        "env = gym.make(env_name)\n",
        "env = gym.wrappers.Monitor(env, \"./video\", video_callable=lambda episode_id: episode_id, force=True)"
      ],
      "metadata": {
        "id": "e3wzIo6olbNN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate our algorithm for a few episodes. \n",
        "\n",
        "Note, the video will be available to download in the video folder in Colab for these test runs."
      ],
      "metadata": {
        "id": "IsodFDuoxuLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dqn.test(env, nb_episodes= 5, visualize=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "995d6c2b-2410-422e-9f9e-525683372078",
        "id": "j-b6WN9IlbNO"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 5 episodes ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: reward: -104.000, steps: 104\n",
            "Episode 2: reward: -103.000, steps: 103\n",
            "Episode 3: reward: -86.000, steps: 86\n",
            "Episode 4: reward: -84.000, steps: 84\n",
            "Episode 5: reward: -85.000, steps: 85\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8564314d50>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic RL Model"
      ],
      "metadata": {
        "id": "5NUD-xUS54RD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section implements a basic ML model on the Mountain Gym environment without using any deep learning. This is added here more as a reference, since the scores were not very good, I explored deep learning models that perform better.\n",
        "\n",
        "Code for this section is from [Genevieve Hayes](https://towardsdatascience.com/getting-started-with-reinforcement-learning-and-open-ai-gym-c289aca874f)"
      ],
      "metadata": {
        "id": "b9odAeMa598a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Delete the previous environment"
      ],
      "metadata": {
        "id": "_CUZXq1Gsq0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del env"
      ],
      "metadata": {
        "id": "IxtlB0RhszIl"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import random"
      ],
      "metadata": {
        "id": "NUsXubNd9E4t"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import needed packages"
      ],
      "metadata": {
        "id": "1xTcAUIlvSDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "VTYJ0bhAvUdm"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import and initialize Mountain Car Environment\n",
        "# Load gym environment \n",
        "env_name = 'MountainCar-v0'\n",
        "env = gym.make(env_name)"
      ],
      "metadata": {
        "id": "AvaH64VqBWJg"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Q-learning function\n",
        "def QLearning(env, learning, discount, epsilon, min_eps, episodes):\n",
        "    # Determine size of discretized state space\n",
        "    num_states = (env.observation_space.high - env.observation_space.low)*\\\n",
        "                    np.array([10, 100])\n",
        "    num_states = np.round(num_states, 0).astype(int) + 1\n",
        "    \n",
        "    # Initialize Q table\n",
        "    Q = np.random.uniform(low = -1, high = 1, \n",
        "                          size = (num_states[0], num_states[1], \n",
        "                                  env.action_space.n))\n",
        "    \n",
        "    # Initialize variables to track rewards\n",
        "    reward_list = []\n",
        "    ave_reward_list = []\n",
        "    \n",
        "    # Calculate episodic reduction in epsilon\n",
        "    reduction = (epsilon - min_eps)/episodes\n",
        "    \n",
        "    # Run Q learning algorithm\n",
        "    for i in range(episodes):\n",
        "        # Initialize parameters\n",
        "        done = False\n",
        "        tot_reward, reward = 0,0\n",
        "        state = env.reset()\n",
        "        \n",
        "        # Discretize state\n",
        "        state_adj = (state - env.observation_space.low)*np.array([10, 100])\n",
        "        state_adj = np.round(state_adj, 0).astype(int)\n",
        "    \n",
        "        while done != True:   \n",
        "            # Render environment for last five episodes\n",
        "            # if i >= (episodes - 20):\n",
        "            #   print(f'episode #: {episodes}')\n",
        "            #   env.render()\n",
        "                \n",
        "            # Determine next action - epsilon greedy strategy\n",
        "            if np.random.random() < 1 - epsilon:\n",
        "                action = np.argmax(Q[state_adj[0], state_adj[1]]) \n",
        "            else:\n",
        "                action = np.random.randint(0, env.action_space.n)\n",
        "                \n",
        "            # Get next state and reward\n",
        "            state2, reward, done, info = env.step(action) \n",
        "            \n",
        "            # Discretize state2\n",
        "            state2_adj = (state2 - env.observation_space.low)*np.array([10, 100])\n",
        "            state2_adj = np.round(state2_adj, 0).astype(int)\n",
        "            \n",
        "            #Allow for terminal states\n",
        "            if done and state2[0] >= 0.5:\n",
        "                Q[state_adj[0], state_adj[1], action] = reward\n",
        "                \n",
        "            # Adjust Q value for current state\n",
        "            else:\n",
        "                delta = learning*(reward + \n",
        "                                 discount*np.max(Q[state2_adj[0], \n",
        "                                                   state2_adj[1]]) - \n",
        "                                 Q[state_adj[0], state_adj[1],action])\n",
        "                Q[state_adj[0], state_adj[1],action] += delta\n",
        "                                     \n",
        "            # Update variables\n",
        "            tot_reward += reward\n",
        "            state_adj = state2_adj\n",
        "        \n",
        "        # Decay epsilon\n",
        "        if epsilon > min_eps:\n",
        "            epsilon -= reduction\n",
        "        \n",
        "        # Track rewards\n",
        "        reward_list.append(tot_reward)\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            ave_reward = np.mean(reward_list)\n",
        "            ave_reward_list.append(ave_reward)\n",
        "            reward_list = []\n",
        "            \n",
        "        if (i+1) % 100 == 0:    \n",
        "            print('Episode {} Average Reward: {}'.format(i+1, ave_reward))\n",
        "            \n",
        "    env.close()\n",
        "    \n",
        "    return ave_reward_list\n",
        "\n",
        "# Run Q-learning algorithm\n",
        "rewards = QLearning(env, 0.2, 0.9, 0.8, 0, 5_000)\n",
        "\n",
        "# Plot Rewards\n",
        "plt.plot(100*(np.arange(len(rewards)) + 1), rewards)\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Average Reward')\n",
        "plt.title('Average Reward vs Episodes')\n",
        "plt.savefig('rewards.jpg')     \n",
        "plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e247159-ac01-4032-b28a-1325a9deb32c",
        "id": "TOpUqcyA6wMT"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100 Average Reward: -200.0\n",
            "Episode 200 Average Reward: -200.0\n",
            "Episode 300 Average Reward: -200.0\n",
            "Episode 400 Average Reward: -200.0\n",
            "Episode 500 Average Reward: -200.0\n",
            "Episode 600 Average Reward: -200.0\n",
            "Episode 700 Average Reward: -200.0\n",
            "Episode 800 Average Reward: -200.0\n",
            "Episode 900 Average Reward: -200.0\n",
            "Episode 1000 Average Reward: -200.0\n",
            "Episode 1100 Average Reward: -200.0\n",
            "Episode 1200 Average Reward: -200.0\n",
            "Episode 1300 Average Reward: -200.0\n",
            "Episode 1400 Average Reward: -200.0\n",
            "Episode 1500 Average Reward: -200.0\n",
            "Episode 1600 Average Reward: -200.0\n",
            "Episode 1700 Average Reward: -200.0\n",
            "Episode 1800 Average Reward: -200.0\n",
            "Episode 1900 Average Reward: -200.0\n",
            "Episode 2000 Average Reward: -200.0\n",
            "Episode 2100 Average Reward: -200.0\n",
            "Episode 2200 Average Reward: -200.0\n",
            "Episode 2300 Average Reward: -200.0\n",
            "Episode 2400 Average Reward: -200.0\n",
            "Episode 2500 Average Reward: -200.0\n",
            "Episode 2600 Average Reward: -200.0\n",
            "Episode 2700 Average Reward: -200.0\n",
            "Episode 2800 Average Reward: -200.0\n",
            "Episode 2900 Average Reward: -200.0\n",
            "Episode 3000 Average Reward: -199.67\n",
            "Episode 3100 Average Reward: -200.0\n",
            "Episode 3200 Average Reward: -200.0\n",
            "Episode 3300 Average Reward: -200.0\n",
            "Episode 3400 Average Reward: -199.58\n",
            "Episode 3500 Average Reward: -198.79\n",
            "Episode 3600 Average Reward: -199.51\n",
            "Episode 3700 Average Reward: -199.25\n",
            "Episode 3800 Average Reward: -199.85\n",
            "Episode 3900 Average Reward: -198.69\n",
            "Episode 4000 Average Reward: -196.84\n",
            "Episode 4100 Average Reward: -200.0\n",
            "Episode 4200 Average Reward: -198.52\n",
            "Episode 4300 Average Reward: -193.31\n",
            "Episode 4400 Average Reward: -195.11\n",
            "Episode 4500 Average Reward: -194.83\n",
            "Episode 4600 Average Reward: -193.97\n",
            "Episode 4700 Average Reward: -197.71\n",
            "Episode 4800 Average Reward: -185.49\n",
            "Episode 4900 Average Reward: -174.08\n",
            "Episode 5000 Average Reward: -179.04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reference"
      ],
      "metadata": {
        "id": "BdrGirQfzBjK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reviewed Q-policy RL from [Genevieve Hayes](https://towardsdatascience.com/getting-started-with-reinforcement-learning-and-open-ai-gym-c289aca874f)\n",
        "\n",
        "Got the Colab install dependencies and video saving from  [cwkx's video](https://www.youtube.com/watch?v=BNSwFURmaCA&ab_channel=cwkx)\n",
        "\n",
        "RL Overview picture and comments from [sadiakhaf](https://github.com/sadiakhaf/IEEE-Hands-On-RL-using-Python)\n",
        "\n",
        "Sample code for using Keras RL with Mountain Car [aslamplr](https://github.com/aslamplr/mountaincar_gym)"
      ],
      "metadata": {
        "id": "bD8_6BcfzD-w"
      }
    }
  ]
}